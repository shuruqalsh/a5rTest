// Ù‡Ù†Ø§ Ù…ÙˆØ¬ÙˆØ¯ ÙƒÙ„ Ø´ÙŠ ÙŠØ®Øµ ØªØ­Ù„ÙŠÙ„ Ù„ØºØ© Ø§Ù„Ø¬Ø³Ø¯ ÙˆÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù†Ø·Ù‚ Ù„Ø¯ÙˆØ§Ù„




import UIKit
import AVFoundation
import Vision
import CoreGraphics

class PresentationAnalyzerViewModel: NSObject, ObservableObject {
    @Published var positionText: String = ""
    @Published var feedbackText: String = ""
    @Published var cameraError: String = ""
    
    private var leftHipPositions: [CGFloat] = []
    private var rightHipPositions: [CGFloat] = []
    private var leftKneePositions: [CGFloat] = []
    private var rightKneePositions: [CGFloat] = []
    private var kneeDistances: [CGFloat] = []

    
    private var previousLeftWristVelocity: CGFloat = 0
    private var previousRightWristVelocity: CGFloat = 0
    private var lastMovementTimestamp: TimeInterval = 0
    private let movementThreshold: CGFloat = 0.03  // Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØºÙŠØ± Ø§Ù„Ø°ÙŠ ÙŠØªÙ… Ø§Ø¹ØªØ¨Ø§Ø±Ù‡ Ø­Ø±ÙƒØ©
    private let velocityThreshold: CGFloat = 0.02  // Ø­Ø¯ Ø§Ù„Ø³Ø±Ø¹Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø­Ø±ÙƒØ© Ø¨Ø·ÙŠØ¦Ø©
    private var isBodyLanguageUsed = false // Ù„ØªØ®Ø²ÙŠÙ† Ø­Ø§Ù„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© Ø§Ù„Ø¬Ø³Ø¯

    
    private var stableMovementFrames: Int = 0 // Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ Ø§Ø³ØªÙ…Ø±Øª ÙÙŠÙ‡Ø§ Ø§Ù„Ø­Ø±ÙƒØ©
    private var stableThreshold: Int = 5 // Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ÙÙŠÙ‡Ø§ Ø§Ù„Ø­Ø±ÙƒØ© Ø«Ø§Ø¨ØªØ© Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø§Ù„Ø©
    private var previousMovementDelta: CGFloat = 0 // Ù„Ø­ÙØ¸ Ø§Ù„ØªØºÙŠØ± Ø§Ù„Ø³Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ø­Ø±ÙƒØ©

    
    private var wristPositions: [CGPoint] = []

    private var previousLeftWrist: CGPoint?
    private var previousRightWrist: CGPoint?


    private var lastDetectedMovement: CGFloat = 0 // Ù„Ø­ÙØ¸ Ø¢Ø®Ø± Ø­Ø±ÙƒØ© ÙƒØ¨ÙŠØ±Ø© ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§

    
    private var stillnessStartTime: Date?

    
    private var hipPositions: [CGFloat] = []
    private var hipMovements: [CGFloat] = []
    private var stillFrameCount = 0

    
    private var captureSession: AVCaptureSession?
    var videoPreviewLayer: AVCaptureVideoPreviewLayer? {
        didSet {
            if let layer = videoPreviewLayer {
                layer.videoGravity = .resizeAspectFill
            }
        }
    }
    private var neckPositions: [CGPoint] = []
    private var relativeMovements: [CGFloat] = []
    
    private var faceAnalysisRequest: VNDetectFaceLandmarksRequest?
    private let bodyPoseRequest = VNDetectHumanBodyPoseRequest()
    private let sequenceRequestHandler = VNSequenceRequestHandler()
    
    override init() {
        super.init()
        setupVision()
    }
    
    func setupCamera(in view: UIView) {
        // Ensure we're on the main thread
        DispatchQueue.main.async {
            self.setupCameraInternal(in: view)
        }
    }

    private func setupCameraInternal(in view: UIView) {
        // Stop existing session if any
        captureSession?.stopRunning()
        
        // Create new session
        let session = AVCaptureSession()
        session.beginConfiguration()
        
        // Set quality level
        if session.canSetSessionPreset(.high) {
            session.sessionPreset = .high
        }
        
        // Setup camera input
        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else {
            self.cameraError = "No front camera available"
            return
        }
        
        do {
            let input = try AVCaptureDeviceInput(device: camera)
            if session.canAddInput(input) {
                session.addInput(input)
            } else {
                self.cameraError = "Could not add camera input"
                return
            }
            
            // Setup video output
            let videoOutput = AVCaptureVideoDataOutput()
            videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))
            if session.canAddOutput(videoOutput) {
                session.addOutput(videoOutput)
            } else {
                self.cameraError = "Could not add video output"
                return
            }
            
            // Commit configuration
            session.commitConfiguration()
            
            // Setup preview layer
            let previewLayer = AVCaptureVideoPreviewLayer(session: session)
            previewLayer.frame = view.bounds
            previewLayer.videoGravity = .resizeAspectFill
            
            // Remove existing preview layer if any
            view.layer.sublayers?.forEach { layer in
                if layer is AVCaptureVideoPreviewLayer {
                    layer.removeFromSuperlayer()
                }
            }
            
            // Add new preview layer
            view.layer.insertSublayer(previewLayer, at: 0)
            self.videoPreviewLayer = previewLayer
            self.captureSession = session
            
            // Start running
            DispatchQueue.global(qos: .userInitiated).async {
                session.startRunning()
            }
            
        } catch {
            self.cameraError = "Camera setup error: \(error.localizedDescription)"
            print("Camera setup error: \(error)")
        }
    }

    
    
    private func setupVision() {
        faceAnalysisRequest = VNDetectFaceLandmarksRequest { [weak self] request, error in
            if let error = error {
                print("Face detection error: \(error.localizedDescription)")
                return
            }
            self?.handleFaceDetectionResults(request)
        }
    }
    
    private func handleFaceDetectionResults(_ request: VNRequest) {
        guard let observations = request.results as? [VNFaceObservation] else { return }
        
        DispatchQueue.main.async { [weak self] in
            if let face = observations.first {
                let pitch = face.pitch?.doubleValue ?? 0
                let yaw = face.yaw?.doubleValue ?? 0
                
                if abs(pitch) > 0.5 || abs(yaw) > 0.5 {
                    self?.feedbackText = "Ø­Ø±ÙƒØ© Ø§Ù„Ø±Ø£Ø³: Ø­Ø§ÙˆÙ„ ØªØ«Ø¨ÙŠØª Ø±Ø£Ø³Ùƒ Ø£ÙƒØ«Ø±"
                } else {
                    self?.feedbackText = "Ø­Ø±ÙƒØ© Ø§Ù„Ø±Ø£Ø³: Ø¬ÙŠØ¯Ø©"
                }
            }
        }
    }
    
    private func analyzeBodyPose(_ pixelBuffer: CVPixelBuffer) {
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        
        do {
            try handler.perform([bodyPoseRequest])
            guard let observation = bodyPoseRequest.results?.first else { return }
            
            let points = try observation.recognizedPoints(.all)
            if let wrist = points[.rightWrist],
               let neck = points[.neck],
               let elbow = points[.rightElbow] {
                
                
                // âœ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© ØªØ­Ù„ÙŠÙ„ Ø­Ø±ÙƒØ© Ø±Ù‚Ù… 3
                detectBodyLanguageUsage(points)
                
                let distanceToNeck = hypot(wrist.location.x - neck.location.x, wrist.location.y - neck.location.y)
                let elbowBend = abs(elbow.location.y - wrist.location.y)
                
                if distanceToNeck < 0.1 && elbowBend > 0.1 {
                    trackWristAndNeckPosition(wrist.location, neck.location) // âœ… ØªÙ…Ø±ÙŠØ± ÙƒÙ„Ù‹Ø§ Ù…Ù† Ø§Ù„ÙŠØ¯ ÙˆØ§Ù„Ø±Ù‚Ø¨Ø©
                } else {
                    
                    wristPositions.removeAll() // â­ï¸ Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· Ø§Ù„Ù…ØµÙÙˆÙØ© Ø¹Ù†Ø¯ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ
                }
            }
        } catch {
            print("Error analyzing body pose: \(error.localizedDescription)")
        }
    }

    private func trackWristAndNeckPosition(_ wrist: CGPoint, _ neck: CGPoint) {
        wristPositions.append(wrist)
        neckPositions.append(neck)
        
        if wristPositions.count > 45 {
            wristPositions.removeFirst()
        }
        if neckPositions.count > 45 {
            neckPositions.removeFirst()
        }
        
        // â­ï¸ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„ÙŠØ¯ ÙˆØ§Ù„Ø±Ù‚Ø¨Ø© Ø¹Ø¨Ø± ÙƒÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø©
        let relativeDistances = zip(wristPositions, neckPositions).map { distanceBetweenPoints($0, $1) }
        
        // â­ï¸ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ù„Ø­Ø¸ÙŠ ÙÙŠ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©
        let currentRelativeMovement = abs(relativeDistances.last ?? 0.0 - (relativeDistances.dropLast().last ?? 0.0))
        
        // â­ï¸ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…ØµÙÙˆÙØ©
        relativeMovements.append(currentRelativeMovement)
        if relativeMovements.count > 15 { // Ø­ÙØ¸ Ø¢Ø®Ø± 15 Ø­Ø±ÙƒØ© Ù†Ø³Ø¨ÙŠØ© ÙÙ‚Ø·
            relativeMovements.removeFirst()
        }
        
        // â­ï¸ Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        let averageRelativeMovement = relativeMovements.reduce(0.0, +) / CGFloat(relativeMovements.count)
        
        print("ğŸ“Š Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù„Ø­Ø¸ÙŠØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©: \(currentRelativeMovement), Ù…ØªÙˆØ³Ø· Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©: \(averageRelativeMovement)")

        // â­ï¸ Ù…Ù†Ø·Ù‚ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¨Ù…Ø±ÙˆØ± Ø§Ù„ÙˆÙ‚Øª
        if averageRelativeMovement < 0.02 { // ğŸ”¥ Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ù…Ø®ÙØ¶Ø© Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
            print("ğŸ›‘ Ø§Ù„ÙŠØ¯ Ø«Ø§Ø¨ØªØ© Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù‚Ø¨Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­Ø±ÙƒØ©")
            updatePositionLabel("ğŸ›‘ Ø§Ù„ÙŠØ¯ Ø«Ø§Ø¨ØªØ© Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù‚Ø¨Ø©")
        } else {
            updatePositionLabel("ğŸ›‘ Ø§Ù„ÙŠØ¯ Ø«Ø§Ø¨ØªØ© Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù‚Ø¨Ø©")
        }
    }
    

    private func detectBodyLanguageUsage(_ points: [VNHumanBodyPoseObservation.JointName: VNRecognizedPoint]) {
        let stableThreshold = 40 // Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
        
        guard let leftWrist = points[.leftWrist],
              let rightWrist = points[.rightWrist] else {
            print("âŒ Ø§Ù„Ù…Ø¹ØµÙ…Ø§Ù† ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ÙŠÙ†")
            return
        }

        // Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØºÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¹ØµÙ…ÙŠÙ† ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ ÙˆØ§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø³Ø§Ø¨Ù‚
        let leftWristDelta = distanceBetweenPoints(leftWrist.location, previousLeftWrist ?? leftWrist.location)
        let rightWristDelta = distanceBetweenPoints(rightWrist.location, previousRightWrist ?? rightWrist.location)

        // Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØºÙŠØ± Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙÙŠ Ø§Ù„Ø­Ø±ÙƒØ§Øª
        let totalMovementDelta = leftWristDelta + rightWristDelta
        
        // Ø­Ø³Ø§Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø© Ø¹Ø¨Ø± Ø§Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        let currentTime = Date().timeIntervalSince1970
        let deltaTime = currentTime - lastMovementTimestamp
        let leftWristVelocity = leftWristDelta / CGFloat(deltaTime)
        let rightWristVelocity = rightWristDelta / CGFloat(deltaTime)
        
        // ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø­Ø±ÙƒØ© Ø¨Ø·ÙŠØ¦Ø©
        let isMovingSlowly = leftWristVelocity < velocityThreshold && rightWristVelocity < velocityThreshold
        
        // Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØºÙŠØ± ÙÙŠ Ø§Ù„Ø­Ø±ÙƒØ© Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„Ø¹ØªØ¨Ø© Ùˆ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ø­Ø±ÙƒØ© Ø¨Ø·ÙŠØ¦Ø©
        let isUsingBodyLanguage = totalMovementDelta > movementThreshold && !isMovingSlowly

        if isUsingBodyLanguage {
            if !isBodyLanguageUsed {
                updatePositionLabel("ğŸ‘ Ø§Ù„Ø´Ø®Øµ ÙŠØ³ØªØ®Ø¯Ù… Ù„ØºØ© Ø§Ù„Ø¬Ø³Ø¯")
                isBodyLanguageUsed = true
                stableMovementFrames = 0 // Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¥Ø°Ø§ Ø¨Ø¯Ø£Øª Ø§Ù„Ø­Ø±ÙƒØ©
            }
        } else {
            stableMovementFrames += 1
            if stableMovementFrames >= stableThreshold {
                if isBodyLanguageUsed {
                    updatePositionLabel("âœ‹ Ø§Ù„Ø´Ø®Øµ Ù„Ø§ ÙŠØ³ØªØ®Ø¯Ù… Ù„ØºØ© Ø§Ù„Ø¬Ø³Ø¯")
                    isBodyLanguageUsed = false
                }
            }
        }
        
        // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª ÙˆØ§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ø¥Ø·Ø§Ø± Ø§Ù„ØªØ§Ù„ÙŠ
        previousLeftWrist = leftWrist.location
        previousRightWrist = rightWrist.location
        lastMovementTimestamp = currentTime
    }

    // â­ï¸ Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† Ù†Ù‚Ø·ØªÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ø¥Ù‚Ù„ÙŠØ¯ÙŠØ©
    private func distanceBetweenPoints(_ point1: CGPoint, _ point2: CGPoint) -> CGFloat {
        return hypot(point1.x - point2.x, point1.y - point2.y)
    }



   

    
    private func updatePositionLabel(_ position: String) {
        DispatchQueue.main.async { [weak self] in
            self?.positionText = position
        }
    }

    
      
}

extension PresentationAnalyzerViewModel: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        do {
            try sequenceRequestHandler.perform([faceAnalysisRequest].compactMap { $0 }, on: pixelBuffer, orientation: .up)
            analyzeBodyPose(pixelBuffer)
        } catch {
            print("Failed to perform Vision request: \(error.localizedDescription)")
        }
    }
}

// Ù‡Ù†Ø§ Ù…ÙˆØ¬ÙˆØ¯ ÙƒÙ„ Ø´ÙŠ ÙŠØ®Øµ ØªØ­Ù„ÙŠÙ„ Ù„ØºØ© Ø§Ù„Ø¬Ø³Ø¯ ÙˆÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù†Ø·Ù‚ Ù„Ø¯ÙˆØ§Ù„




import UIKit
import AVFoundation
import Vision
import CoreGraphics

class PresentationAnalyzerViewModel: NSObject, ObservableObject {
    // Ø§Ù„Ù„ÙŠØ¨Ù„Ø² Ù„ÙƒÙ„ ÙˆØ¶Ø¹ÙŠÙ‡
    @Published var positionText: String = ""
    @Published var feedbackText: String = ""
    @Published var handMovementText: String = ""  // Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ¹Ù„Ù‚ Ø¨Ø­Ø±ÙƒØ© Ø§Ù„ÙŠØ¯
    @Published var videoURL: URL? // âœ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…ØªØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯

    private var outputURL: URL?
    private var videoOutput: AVCaptureMovieFileOutput?
      private var captureSession: AVCaptureSession?
      private var videoPreviewLayer: AVCaptureVideoPreviewLayer?
      
      override init() {
          super.init()
          setupVision()
      }
    
    // ğŸ†• Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙˆØ¶Ø¹ÙŠØ© Ø§Ù„Ø±Ù‚Ø¨Ø© Ø¯Ø§Ù„Ø© (Ø§Ù„Ø¨Ø¯Ø§ÙŠÙ‡)
    private var isHandOnNeck: Bool = false
    private var stableFrameCount: Int = 0
    private let requiredStableFrames: Int = 30
    var neckTouchCount: Int = 0// Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Øª Ø§Ù„ÙŠ Ø§Ù„Ø´Ø®Øµ Ù…Ø³Ùƒ ÙÙŠÙ‡Ø§ Ø±Ù‚Ø¨ØªÙ‡
    
    // ğŸ†• Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ØªØªØ¨Ø¹ Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©
    private var wristPositions: [CGPoint] = []
    private var neckPositions: [CGPoint] = []
    private var relativeMovements: [CGFloat] = []
    // ğŸ†• Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙˆØ¶Ø¹ÙŠØ© Ø§Ù„Ø±Ù‚Ø¨Ø© Ø¯Ø§Ù„Ø© (Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡)
    
    // ğŸ†• Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙˆØ¶Ø¹ÙŠØ© Ø§Ù„Ø±Ø£Ø³  (Ø§Ù„Ø¨Ø¯Ø§ÙŠÙ‡)
    private var stableHeadFrameCount: Int = 0
    private var requiredStableHeadFrames: Int = 30 // Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„Ø«Ø¨Ø§Øª
    private var isHeadStable: Bool = true
    @Published var headMovementCount: Int = 0 // ğŸ†• Ø¹Ø¯Ø§Ø¯ Ø®Ø§Øµ Ø¨Ø­Ø±ÙƒØ© Ø§Ù„Ø±Ø£Ø³
    // ğŸ†• Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙˆØ¶Ø¹ÙŠØ© Ø§Ù„Ø±Ø£Ø³  (Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡)
    
    
    
    // ğŸ†• Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙˆØ¶Ø¹ÙŠØ© Ø¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© Ø§Ù„Ø¬Ø³Ø¯  (Ø§Ù„Ø¨Ø¯Ø§ÙŠÙ‡)
    
    // Ù…ØªØºÙŠØ±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø­Ø±ÙƒØ© Ø§Ù„Ù…Ø¹ØµÙ…
    private var lastWristMovement: TimeInterval = 0  // Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ø°ÙŠ ØªÙ… ÙÙŠÙ‡ Ø¢Ø®Ø± Ø­Ø±ÙƒØ©
    private let handRestThreshold: TimeInterval = 5.0 // Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† Ø§Ù„ÙˆÙ‚Øª (Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ) Ø§Ù„Ø°ÙŠ ÙŠØ¹ØªØ¨Ø± ÙÙŠÙ‡ Ø§Ù„Ù…Ø¹ØµÙ… Ø«Ø§Ø¨ØªÙ‹Ø§
    private var handIsNotUsed: Bool = false // Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    
    // ğŸ†• Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙˆØ¶Ø¹ÙŠØ© Ø¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© Ø§Ù„Ø¬Ø³Ø¯  (Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡)
    
    
    
    
    // ğŸ†• Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙˆØ¶Ø¹ÙŠØ©   (Ø§Ù„Ø¨Ø¯Ø§ÙŠÙ‡)Arms Crossed

    private var isArmsCrossed: Bool = false

    

    // ğŸ†• Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡)Arms Crossed

    private var previousLeftWristVelocity: CGFloat = 0
    private var previousRightWristVelocity: CGFloat = 0
    private var lastMovementTimestamp: TimeInterval = 0
    private let movementThreshold: CGFloat = 0.03  // Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØºÙŠØ± Ø§Ù„Ø°ÙŠ ÙŠØªÙ… Ø§Ø¹ØªØ¨Ø§Ø±Ù‡ Ø­Ø±ÙƒØ©
    private let velocityThreshold: CGFloat = 0.02  // Ø­Ø¯ Ø§Ù„Ø³Ø±Ø¹Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø­Ø±ÙƒØ© Ø¨Ø·ÙŠØ¦Ø©
    private var isBodyLanguageUsed = false // Ù„ØªØ®Ø²ÙŠÙ† Ø­Ø§Ù„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© Ø§Ù„Ø¬Ø³Ø¯
    
    
    private var stableMovementFrames: Int = 0 // Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ Ø§Ø³ØªÙ…Ø±Øª ÙÙŠÙ‡Ø§ Ø§Ù„Ø­Ø±ÙƒØ©
    private var stableThreshold: Int = 5 // Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ÙÙŠÙ‡Ø§ Ø§Ù„Ø­Ø±ÙƒØ© Ø«Ø§Ø¨ØªØ© Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø§Ù„Ø©
    private var previousMovementDelta: CGFloat = 0 // Ù„Ø­ÙØ¸ Ø§Ù„ØªØºÙŠØ± Ø§Ù„Ø³Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ø­Ø±ÙƒØ©
    
    
    
    private var previousLeftWrist: CGPoint?
    private var previousRightWrist: CGPoint?
    
    
    private var lastDetectedMovement: CGFloat = 0 // Ù„Ø­ÙØ¸ Ø¢Ø®Ø± Ø­Ø±ÙƒØ© ÙƒØ¨ÙŠØ±Ø© ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§
    
    
    private var stillnessStartTime: Date?
    
    
    private var hipPositions: [CGFloat] = []
    private var hipMovements: [CGFloat] = []
    private var stillFrameCount = 0
    
    
    
    private var faceAnalysisRequest: VNDetectFaceLandmarksRequest?
    private let bodyPoseRequest = VNDetectHumanBodyPoseRequest()
    private let sequenceRequestHandler = VNSequenceRequestHandler()
 
    
    func setupCamera(in view: UIView) {
        captureSession = AVCaptureSession()
        guard let captureSession = captureSession else { return }
        
        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else {
            print("âŒ Failed to access front camera")
            return
        }
        
        do {
            let input = try AVCaptureDeviceInput(device: camera)
            if captureSession.canAddInput(input) {
                captureSession.addInput(input)
            }
            
            let videoOutput = AVCaptureMovieFileOutput() // ğŸ¥ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù€ Movie Output Ù„Ù„ØªØ³Ø¬ÙŠÙ„
            if captureSession.canAddOutput(videoOutput) {
                captureSession.addOutput(videoOutput)
                self.videoOutput = videoOutput
            }
            
            let videoDataOutput = AVCaptureVideoDataOutput()
            videoDataOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))
            if captureSession.canAddOutput(videoDataOutput) {
                captureSession.addOutput(videoDataOutput)
            }
            
            view.layer.sublayers?.forEach { $0.removeFromSuperlayer() }
            
            let videoPreviewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
            videoPreviewLayer.videoGravity = .resizeAspectFill
            
            DispatchQueue.main.async {
                videoPreviewLayer.frame = view.bounds
                view.layer.addSublayer(videoPreviewLayer)
            }
            
            DispatchQueue.global(qos: .userInitiated).async {
                captureSession.startRunning()
            }
            
        } catch {
            print("âŒ Error setting up camera: \(error.localizedDescription)")
        }
    }

    
    
    private func setupVision() {
        faceAnalysisRequest = VNDetectFaceLandmarksRequest { [weak self] request, error in
            if let error = error {
                print("Face detection error: \(error.localizedDescription)")
                return
            }
            self?.handleFaceDetectionResults(request)
        }
    }
    
    private func analyzeBodyPose(_ pixelBuffer: CVPixelBuffer) {
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        
        do {
            try handler.perform([bodyPoseRequest])
            guard let observation = bodyPoseRequest.results?.first else { return }
            
            // ğŸ”¥ ØªØ¹Ø±ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ Landmarks Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¬Ø³Ù…
            let points = try observation.recognizedPoints(.all)
            
            // âœ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØªÙ…Ø±ÙŠØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·
            analyzeNeckPosition(points)
            analyzeHandMovement(points)
            // âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø°Ø±Ø¹ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© ÙÙ‚Ø·
            _ = areArmsCrossedUsingGeometry(points)
            
            
            
        } catch {
            print("Error analyzing body pose: \(error.localizedDescription)")
        }
    }
    
    
    private func handleFaceDetectionResults(_ request: VNRequest) {
        guard let observations = request.results as? [VNFaceObservation] else { return }
        
        DispatchQueue.main.async { [weak self] in
            if let face = observations.first {
                let pitch = face.pitch?.doubleValue ?? 0
                let yaw = face.yaw?.doubleValue ?? 0
                
                // âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø¯Ù‰ Ø«Ø¨Ø§Øª Ø§Ù„Ø±Ø£Ø³ (ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„ØµØºÙŠØ±Ø©)
                if abs(pitch) > 0.5 || abs(yaw) > 0.5 {
                    self?.stableHeadFrameCount += 1
                    print("ğŸ•’ Ø¥Ø·Ø§Ø±Ø§Øª Ø­Ø±ÙƒØ© Ø§Ù„Ø±Ø£Ø³: \(self?.stableHeadFrameCount ?? 0)/\(self?.requiredStableHeadFrames ?? 0)")
                    
                    if self?.stableHeadFrameCount ?? 0 >= self?.requiredStableHeadFrames ?? 0 {
                        if self?.isHeadStable ?? true {
                            self?.isHeadStable = false
                            self?.headMovementCount += 1 // ğŸ†• Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯ Ø¹Ù†Ø¯ Ø§ÙƒØªØ´Ø§Ù Ø­Ø±ÙƒØ© Ø§Ù„Ø±Ø£Ø³ Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
                            self?.feedbackText = "Ø­Ø±ÙƒØ© Ø§Ù„Ø±Ø£Ø³: Ø­Ø§ÙˆÙ„ ØªØ«Ø¨ÙŠØª Ø±Ø£Ø³Ùƒ Ø£ÙƒØ«Ø± (\(self?.headMovementCount ?? 0) Ù…Ø±Ø§Øª)"
                            print("ğŸš¨ Ø§Ù„Ø±Ø£Ø³ ÙŠØªØ­Ø±Ùƒ ÙƒØ«ÙŠØ±Ù‹Ø§! Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Øª: \(self?.headMovementCount ?? 0)")
                        }
                    }
                } else {
                    self?.resetHeadStabilityState()
                }
            }
        }
    }
    
    private func resetHeadStabilityState() {
        stableHeadFrameCount = 0
        isHeadStable = true
        feedbackText = "Ø­Ø±ÙƒØ© Ø§Ù„Ø±Ø£Ø³: Ø¬ÙŠØ¯Ø©"
    }
    
    
    
    
    func analyzeNeckPosition(_ points: [VNHumanBodyPoseObservation.JointName : VNRecognizedPoint]) {
        guard let rightWrist = points[.rightWrist],
              let leftWrist = points[.leftWrist],
              let neck = points[.neck],
              let rightElbow = points[.rightElbow],
              let leftElbow = points[.leftElbow] else {
            resetNeckPositionState()
            return
        }
        
        // ğŸ“ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ²Ø§ÙˆÙŠØ© Ø§Ù„Ø§Ù†Ø­Ù†Ø§Ø¡ Ù„ÙƒÙ„ Ù…Ù† Ø§Ù„ÙŠØ¯ Ø§Ù„ÙŠÙ…Ù†Ù‰ ÙˆØ§Ù„ÙŠØ³Ø±Ù‰
        let rightDistanceToNeck = hypot(rightWrist.location.x - neck.location.x, rightWrist.location.y - neck.location.y)
        let rightElbowBend = abs(rightElbow.location.y - rightWrist.location.y)
        
        let leftDistanceToNeck = hypot(leftWrist.location.x - neck.location.x, leftWrist.location.y - neck.location.y)
        let leftElbowBend = abs(leftElbow.location.y - leftWrist.location.y)
        
        // âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒÙ„ØªØ§ Ø§Ù„ÙŠØ¯ÙŠÙ† Ø¨Ù†ÙØ³ Ø§Ù„Ø´Ø±ÙˆØ·
        let isRightHandOnNeck = rightDistanceToNeck < 0.1 && rightElbowBend > 0.1
        let isLeftHandOnNeck = leftDistanceToNeck < 0.1 && leftElbowBend > 0.1
        
        // âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø«Ø¨Ø§Øª Ù„Ø¹Ø¯Ø¯ ÙƒØ§ÙÙŠ Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
        if (isRightHandOnNeck || isLeftHandOnNeck) {
            stableFrameCount += 1
            print("ğŸ•’ Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ø«Ø¨Ø§Øª: \(stableFrameCount)/\(requiredStableFrames)")
            
            if stableFrameCount >= requiredStableFrames && !isHandOnNeck {
                neckTouchCount += 1
                isHandOnNeck = true
                
                // ğŸ†• ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙˆÙˆØ¶Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
                let handUsed = isRightHandOnNeck ? "Ø§Ù„ÙŠÙ…Ù†Ù‰" : "Ø§Ù„ÙŠØ³Ø±Ù‰"
                
                DispatchQueue.main.async {
                    self.positionText = "ğŸ›‘ Ø§Ù„ÙŠØ¯ \(handUsed) Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù‚Ø¨Ø© (\(self.neckTouchCount) Ù…Ø±Ø§Øª)"
                    self.feedbackText = "ØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙˆØ¶Ø¹ Ø§Ù„ÙŠØ¯ \(handUsed) Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù‚Ø¨Ø©"
                }
                print("ğŸ›‘ Ø§Ù„ÙŠØ¯ \(handUsed) Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù‚Ø¨Ø© - Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Øª: \(neckTouchCount)")
            }
        } else {
            resetNeckPositionState()
        }
    }
    
    private func resetNeckPositionState() {
        stableFrameCount = 0
        isHandOnNeck = false
        DispatchQueue.main.async {
            self.positionText = ""
            self.feedbackText = ""
        }
    }
    
    
    
    
    
    private func analyzeHandMovement(_ points: [VNHumanBodyPoseObservation.JointName: VNRecognizedPoint]) {
        guard let leftWrist = points[.leftWrist], let rightWrist = points[.rightWrist] else { return }
        
        let currentTime = Date().timeIntervalSince1970
        let leftWristMovement = distanceBetweenPoints(previousLeftWrist ?? leftWrist.location, leftWrist.location)
        let rightWristMovement = distanceBetweenPoints(previousRightWrist ?? rightWrist.location, rightWrist.location)
        
        // Ù‚ÙŠØ§Ø³ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø­Ø±ÙƒØ© ØµØºÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ (Ø£ÙŠ Ø§Ù„Ù…Ø¹ØµÙ… Ø«Ø§Ø¨Øª):
        if leftWristMovement < 0.02 && rightWristMovement < 0.02 {
            if currentTime - lastWristMovement > handRestThreshold {
                // Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙŠØ¯ Ø«Ø§Ø¨ØªØ© Ù„ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø©
                if !handIsNotUsed {
                    handIsNotUsed = true
                    DispatchQueue.main.async {
                        self.handMovementText = "Ø§Ù„ÙŠØ¯ ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©. Ø­Ø§ÙˆÙ„ ØªØ­Ø±ÙŠÙƒ ÙŠØ¯Ùƒ!"
                        self.feedbackText = "Ø§Ù„ÙŠØ¯ Ø«Ø§Ø¨ØªØ© Ù„ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø©."
                    }
                    print("ğŸš¨ Ø§Ù„ÙŠØ¯ Ù„Ù… ØªØªØ­Ø±Ùƒ Ù„ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø©!")
                }
            }
        } else {
            lastWristMovement = currentTime
            handIsNotUsed = false
            DispatchQueue.main.async {
                self.handMovementText = ""
            }
        }
        
        // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ø¹ØµÙ…ÙŠÙ†
        previousLeftWrist = leftWrist.location
        previousRightWrist = rightWrist.location
    }
    
    
    func areArmsCrossedUsingGeometry(_ points: [VNHumanBodyPoseObservation.JointName : VNRecognizedPoint]) -> Bool {
        
        // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: Ø§Ù„Ù…Ø±ÙÙ‚ÙŠÙ† ÙˆØ§Ù„Ø£ÙƒØªØ§Ù
        guard let leftElbow = points[.leftElbow],
              let rightElbow = points[.rightElbow],
              let leftShoulder = points[.leftShoulder],
              let rightShoulder = points[.rightShoulder] else {
            print("Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø°Ø±Ø¹ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø©")
            self.positionText = "Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø°Ø±Ø¹ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø©"
            return false
        }
        
        // ØªÙ‚Ø¯ÙŠØ± Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø¹ØµÙ…ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø³ØªÙŠÙØ§Ø¡ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ
        let estimatedLeftWrist = estimateWristPosition(elbow: leftElbow.location, shoulder: leftShoulder.location)
        let estimatedRightWrist = estimateWristPosition(elbow: rightElbow.location, shoulder: rightShoulder.location)
        
        // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ØªÙ‚Ø§Ø·Ø¹ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø­Ø±Ù X
        let isCrossed = estimatedLeftWrist.x > rightElbow.location.x &&
                        estimatedRightWrist.x < leftElbow.location.x
        
        if isCrossed {
            self.positionText = "Ø§Ù„Ø£ÙŠØ¯ÙŠ Ù…ÙƒØªÙ‘ÙØ© (Crossed Arms) âœ…"
        } else {
        }
        
        print(positionText)
        return isCrossed
    }


    // Ø¯Ø§Ù„Ø© ØªÙ‚Ø¯ÙŠØ± Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø¹ØµÙ… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø³ØªÙŠÙØ§Ø¡ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ
    func estimateWristPosition(elbow: CGPoint, shoulder: CGPoint) -> CGPoint {
        let dx = elbow.x - shoulder.x
        let dy = elbow.y - shoulder.y
        return CGPoint(x: elbow.x + dx, y: elbow.y + dy)
    }

    // â­ï¸ Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† Ù†Ù‚Ø·ØªÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ø¥Ù‚Ù„ÙŠØ¯ÙŠØ©
    private func distanceBetweenPoints(_ point1: CGPoint, _ point2: CGPoint) -> CGFloat {
        return hypot(point1.x - point2.x, point1.y - point2.y)
    }



    private func calculateAngleBetweenPoints(shoulder: CGPoint, elbow: CGPoint, wrist: CGPoint) -> CGFloat {
        // Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø§Ø·
        let vector1 = CGVector(dx: elbow.x - shoulder.x, dy: elbow.y - shoulder.y)
        let vector2 = CGVector(dx: wrist.x - elbow.x, dy: wrist.y - elbow.y)
        
        // Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¬Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù‚Ø·ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
        let dotProduct = vector1.dx * vector2.dx + vector1.dy * vector2.dy
        let magnitude1 = sqrt(vector1.dx * vector1.dx + vector1.dy * vector1.dy)
        let magnitude2 = sqrt(vector2.dx * vector2.dx + vector2.dy * vector2.dy)
        
        // Ø­Ø³Ø§Ø¨ Ø§Ù„Ø²Ø§ÙˆÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
        let angle = acos(dotProduct / (magnitude1 * magnitude2)) * 180 / .pi
        
        return angle.isNaN ? 0 : angle
    }

    func stopCamera() { // â­ï¸ Ø¯Ø§Ù„Ø© Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¹Ù†Ø¯ Ø¹Ø¯Ù… Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„ÙŠÙ‡Ø§
        captureSession?.stopRunning()
        captureSession = nil
    }

    
    private func updatePositionLabel(_ position: String) {
        DispatchQueue.main.async { [weak self] in
            self?.positionText = position
        }
    }

    

    
    func startRecording() {
        let tempDirectory = URL(fileURLWithPath: NSTemporaryDirectory(), isDirectory: true)
        let outputURL = tempDirectory.appendingPathComponent(UUID().uuidString).appendingPathExtension("mov")
        
        self.videoURL = outputURL
        
        if let videoOutput = videoOutput, !videoOutput.isRecording {
            videoOutput.startRecording(to: outputURL, recordingDelegate: self)
            print("ğŸ¥ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¥Ù„Ù‰ \(outputURL)")
        }
    }

    func stopRecording() {
        guard let videoOutput = videoOutput else {
            print("âŒ Ø®Ø·Ø£: `videoOutput` ØºÙŠØ± Ù…Ù‡ÙŠØ£!")
            return
        }
        videoOutput.stopRecording()

        }
    }



extension PresentationAnalyzerViewModel: AVCaptureFileOutputRecordingDelegate {
    func fileOutput(_ output: AVCaptureFileOutput, didFinishRecordingTo outputFileURL: URL, from connections: [AVCaptureConnection], error: Error?) {
        if error == nil {
            videoURL = outputFileURL
        } else {
            print("Error recording movie: \(error!.localizedDescription)")
        }
    }
}

extension PresentationAnalyzerViewModel: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        do {
            try sequenceRequestHandler.perform([faceAnalysisRequest].compactMap { $0 }, on: pixelBuffer, orientation: .up)
            analyzeBodyPose(pixelBuffer)
        } catch {
            print("Failed to perform Vision request: \(error.localizedDescription)")
        }
    }
}
